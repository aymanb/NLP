{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Only Books"
      ],
      "metadata": {
        "id": "1O4amuKWrimM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import files # For file upload in Colab\n",
        "\n",
        "#Download NLTK's 'punkt' tokenizer models\n",
        "\n",
        "try:\n",
        "    # Attempt to load the resource directly to check if it exists\n",
        "    nltk.data.find('tokenizers/punkt_tab/english/')\n",
        "except LookupError:\n",
        "    print(\"NLTK 'punkt_tab' resource not found. Downloading...\")\n",
        "    # The traceback suggests 'punkt_tab', so download that specific resource\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"'punkt_tab' downloaded successfully.\")\n",
        "\n",
        "#Upload your CSV file\n",
        "print(\"Please upload your Readwise CSV export file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if a file was uploaded and get its name\n",
        "if not uploaded:\n",
        "    print(\"\\nNo file was uploaded. Please run the cell again and select your CSV file.\")\n",
        "    file_name = None\n",
        "else:\n",
        "    file_name = next(iter(uploaded))\n",
        "    print(f\"\\nSuccessfully uploaded '{file_name}'.\")\n",
        "\n",
        "#Read the CSV into a pandas DataFrame\n",
        "if file_name:\n",
        "    try:\n",
        "        df = pd.read_csv(file_name)\n",
        "        print(\"\\nCSV file loaded successfully into a DataFrame.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError loading CSV file: {e}\")\n",
        "        print(\"Please ensure the uploaded file is a valid CSV and the file name is correct.\")\n",
        "        df = pd.DataFrame()\n",
        "else:\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "#FILTER: Keep only books\n",
        "if not df.empty and 'Amazon Book ID' in df.columns:\n",
        "    df = df[df['Amazon Book ID'].notnull() & (df['Amazon Book ID'] != '')]\n",
        "    print(f\"\\nAfter filtering, {len(df)} rows remain with a non-null Amazon Book ID.\")\n",
        "else:\n",
        "    print(\"\\nWarning: 'Amazon Book ID' column not found. No filter applied.\")\n",
        "\n",
        "# Proceed only if the DataFrame is not empty\n",
        "if not df.empty:\n",
        "    #Inspect the dataframe\n",
        "    print(\"\\n--- DataFrame Inspection ---\")\n",
        "    print(\"last 5 rows of your data:\")\n",
        "    print(df.tail())\n",
        "    print(\"\\nDataFrame Info (columns, data types, non-null counts):\")\n",
        "    df.info()\n",
        "\n",
        "    #Check if you have Higlight and Book Title columns\n",
        "    highlight_column_name = 'Highlight'  #CHANGE THIS IF YOUR COLUMN NAME IS DIFFERENT\n",
        "    book_title_column_name = 'Book Title' #CHANGE THIS IF YOUR COLUMN NAME IS DIFFERENT !!!\n",
        "\n",
        "\n",
        "    # Check if the specified highlight column exists\n",
        "    if highlight_column_name not in df.columns:\n",
        "        print(f\"\\nError: The column '{highlight_column_name}' was not found in your CSV.\")\n",
        "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "        print(\"Please update the 'highlight_column_name' variable in the script and re-run.\")\n",
        "        df = pd.DataFrame() # Effectively stop processing\n",
        "    else:\n",
        "        print(f\"\\nUsing column '{highlight_column_name}' for highlight analysis.\")\n",
        "\n",
        "    # Check if the specified book title column exists (only if df is still valid)\n",
        "    if not df.empty and book_title_column_name not in df.columns:\n",
        "        print(f\"\\nError: The column '{book_title_column_name}' was not found in your CSV.\")\n",
        "        print(f\"Available columns are: {df.columns.tolist()}\")\n",
        "        print(f\"Please update the 'book_title_column_name' variable in the script and re-run if you want per-book stats.\")\n",
        "        # We can still proceed with overall stats, but per-book stats will be skipped.\n",
        "        can_do_book_stats = False\n",
        "    elif not df.empty:\n",
        "        print(f\"Using column '{book_title_column_name}' for per-book analysis.\")\n",
        "        can_do_book_stats = True\n",
        "    else:\n",
        "        can_do_book_stats = False\n",
        "\n",
        "\n",
        "# Proceed only if the DataFrame and highlight column are valid\n",
        "if not df.empty:\n",
        "    #Basic Text Analytics (per highlight) ---\n",
        "\n",
        "    # Ensure the highlight column is treated as string and handle potential missing values (NaN)\n",
        "    df[highlight_column_name] = df[highlight_column_name].astype(str).fillna('')\n",
        "\n",
        "    #Word Count (simple method: split by space)\n",
        "    df['word_count_simple'] = df[highlight_column_name].apply(lambda x: len(x.split()))\n",
        "\n",
        "    #Token Count (using NLTK's word_tokenize for better accuracy)\n",
        "    df['token_count_nltk'] = df[highlight_column_name].apply(lambda x: len(word_tokenize(x)))\n",
        "\n",
        "    #Display Per-Highlight Results (Sample) ---\n",
        "    print(\"\\n--- Per-Highlight Analytics Results (Sample) ---\")\n",
        "    print(\"First 5 rows with new analytics columns ('word_count_simple', 'token_count_nltk'):\")\n",
        "    print(df[[highlight_column_name, 'word_count_simple', 'token_count_nltk']].head())\n",
        "\n",
        "    #Aggregate Statistics (Overall) ---\n",
        "    print(\"\\n--- Aggregate Statistics (Overall) ---\")\n",
        "    total_highlights = len(df)\n",
        "    total_words_simple = df['word_count_simple'].sum()\n",
        "    total_tokens_nltk = df['token_count_nltk'].sum()\n",
        "\n",
        "    print(f\"Total number of highlights processed: {total_highlights}\")\n",
        "    print(f\"Total estimated words (space-separated): {total_words_simple}\")\n",
        "    print(f\"Total tokens (NLTK): {total_tokens_nltk}\")\n",
        "\n",
        "    if total_highlights > 0:\n",
        "        average_words_per_highlight = df['word_count_simple'].mean()\n",
        "        average_tokens_per_highlight = df['token_count_nltk'].mean()\n",
        "        print(f\"Average words per highlight (simple): {average_words_per_highlight:.2f}\")\n",
        "        print(f\"Average NLTK tokens per highlight: {average_tokens_per_highlight:.2f}\")\n",
        "    else:\n",
        "        print(\"No highlights processed to calculate averages.\")\n",
        "\n",
        "    # Per-Book Title Statistics ---\n",
        "    if can_do_book_stats:\n",
        "        print(\"\\n--- Aggregate Statistics (Per Book Title) ---\")\n",
        "        # Ensure the book title column is treated as string and handle potential missing values\n",
        "        df[book_title_column_name] = df[book_title_column_name].astype(str).fillna('Unknown Title')\n",
        "\n",
        "        # Group by book title and sum the word and token counts\n",
        "        # Removed the 'highlight_id' count from the initial agg call as it caused a KeyError\n",
        "        book_stats = df.groupby(book_title_column_name).agg(\n",
        "            total_words_in_book=('word_count_simple', 'sum'),\n",
        "            total_tokens_in_book=('token_count_nltk', 'sum')\n",
        "        ).reset_index() # Reset index to make 'Book Title' a column again\n",
        "\n",
        "        # Add highlight count separately using the highlight_column_name\n",
        "        highlight_counts = df.groupby(book_title_column_name)[highlight_column_name].count().reset_index(name='total_highlights_in_book')\n",
        "        book_stats = pd.merge(book_stats, highlight_counts, on=book_title_column_name)\n",
        "\n",
        "        print(\"Word and Token Counts Per Book Title:\")\n",
        "        # To display all rows of book_stats if it's long, you might want to adjust pandas display options\n",
        "        # pd.set_option('display.max_rows', None)\n",
        "        print(book_stats.sort_values(by='total_tokens_in_book', ascending=False))\n",
        "        # pd.reset_option('display.max_rows')\n",
        "    else:\n",
        "        print(\"\\nSkipping per-book statistics because the book title column was not found or properly specified.\")\n",
        "\n",
        "\n",
        "\n",
        "else:\n",
        "    if file_name:\n",
        "        print(\"\\nProcessing halted due to issues with the DataFrame or column selection.\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")\n"
      ],
      "metadata": {
        "id": "TWwYZ-dTEyEs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59913efc-81dc-4b62-ca7c-8d8f08a83f1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Readwise CSV export file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8cde7c49-dad8-4646-92ff-d3a1578418d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8cde7c49-dad8-4646-92ff-d3a1578418d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving readwise-data-Nov 5-2024.csv to readwise-data-Nov 5-2024 (1).csv\n",
            "\n",
            "Successfully uploaded 'readwise-data-Nov 5-2024 (1).csv'.\n",
            "\n",
            "CSV file loaded successfully into a DataFrame.\n",
            "\n",
            "After filtering, 24658 rows remain with a non-null Amazon Book ID.\n",
            "\n",
            "--- DataFrame Inspection ---\n",
            "last 5 rows of your data:\n",
            "                                               Highlight  \\\n",
            "28725  Make Your Own Mentors: A PhD from the Universi...   \n",
            "28726  At Oakland, Al Davis introduced me (and anyone...   \n",
            "28727  THE WALSH WAY The Fog Cutter Randy Cross, San ...   \n",
            "28728  Good luck meeting that one every year. Neverth...   \n",
            "28729  PART V Thin Skin, Baloney, and “The Star-Spang...   \n",
            "\n",
            "                                              Book Title  \\\n",
            "28725  The Score Takes Care of Itself: My Philosophy ...   \n",
            "28726  The Score Takes Care of Itself: My Philosophy ...   \n",
            "28727  The Score Takes Care of Itself: My Philosophy ...   \n",
            "28728  The Score Takes Care of Itself: My Philosophy ...   \n",
            "28729  The Score Takes Care of Itself: My Philosophy ...   \n",
            "\n",
            "                                  Book Author Amazon Book ID    Note   Color  \\\n",
            "28725  Bill Walsh, Steve Jamison, Craig Walsh     B002G54Y04     .h2  yellow   \n",
            "28726  Bill Walsh, Steve Jamison, Craig Walsh     B002G54Y04     NaN  yellow   \n",
            "28727  Bill Walsh, Steve Jamison, Craig Walsh     B002G54Y04     .h2  yellow   \n",
            "28728  Bill Walsh, Steve Jamison, Craig Walsh     B002G54Y04  .story  yellow   \n",
            "28729  Bill Walsh, Steve Jamison, Craig Walsh     B002G54Y04     .h1  yellow   \n",
            "\n",
            "        Tags Location Type  Location             Highlighted at Document tags  \n",
            "28725     h2      location    3237.0  2020-10-11 04:56:00+00:00           NaN  \n",
            "28726    NaN      location    3246.0  2020-10-11 04:56:00+00:00           NaN  \n",
            "28727     h2      location    3306.0  2020-10-11 04:56:00+00:00           NaN  \n",
            "28728  story      location    3368.0  2020-10-11 04:56:00+00:00           NaN  \n",
            "28729     h1      location    3392.0  2020-10-11 04:56:00+00:00           NaN  \n",
            "\n",
            "DataFrame Info (columns, data types, non-null counts):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 24658 entries, 0 to 28729\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Highlight       24658 non-null  object \n",
            " 1   Book Title      24658 non-null  object \n",
            " 2   Book Author     24654 non-null  object \n",
            " 3   Amazon Book ID  24658 non-null  object \n",
            " 4   Note            10380 non-null  object \n",
            " 5   Color           24541 non-null  object \n",
            " 6   Tags            11108 non-null  object \n",
            " 7   Location Type   24644 non-null  object \n",
            " 8   Location        24646 non-null  float64\n",
            " 9   Highlighted at  24658 non-null  object \n",
            " 10  Document tags   1812 non-null   object \n",
            "dtypes: float64(1), object(10)\n",
            "memory usage: 2.3+ MB\n",
            "\n",
            "Using column 'Highlight' for highlight analysis.\n",
            "Using column 'Book Title' for per-book analysis.\n",
            "\n",
            "--- Per-Highlight Analytics Results (Sample) ---\n",
            "First 5 rows with new analytics columns ('word_count_simple', 'token_count_nltk'):\n",
            "                                           Highlight  word_count_simple  \\\n",
            "0  CHAPTER 8 THEY DON’T MAKE THE SAME MISTAKES OV...                 11   \n",
            "1  CHAPTER 9 THEY DON’T RESENT OTHER PEOPLE’S SUC...                  8   \n",
            "2  CHAPTER 10 THEY DON’T GIVE UP AFTER THE FIRST ...                 10   \n",
            "3              CHAPTER 11 THEY DON’T FEAR ALONE TIME                  7   \n",
            "4  CHAPTER 12 THEY DON’T FEEL THE WORLD OWES THEM...                 10   \n",
            "\n",
            "   token_count_nltk  \n",
            "0                13  \n",
            "1                12  \n",
            "2                12  \n",
            "3                 9  \n",
            "4                12  \n",
            "\n",
            "--- Aggregate Statistics (Overall) ---\n",
            "Total number of highlights processed: 24658\n",
            "Total estimated words (space-separated): 1700234\n",
            "Total tokens (NLTK): 1984734\n",
            "Average words per highlight (simple): 68.95\n",
            "Average NLTK tokens per highlight: 80.49\n",
            "\n",
            "--- Aggregate Statistics (Per Book Title) ---\n",
            "Word and Token Counts Per Book Title:\n",
            "                                            Book Title  total_words_in_book  \\\n",
            "332     The 33 Strategies of War (Joost Elffers Books)                35924   \n",
            "334                               The 48 Laws of Power                25278   \n",
            "229                                   Napoleon: A Life                24093   \n",
            "443         The Silk Roads: A New History of the World                24678   \n",
            "206  Loonshots: How to Nurture the Crazy Ideas That...                21171   \n",
            "..                                                 ...                  ...   \n",
            "20   A Season with Verona: Travels Around Italy in ...                    2   \n",
            "61   Building the Yellow Wall: The Incredible Rise ...                    2   \n",
            "96              Data Visualization with D3.js Cookbook                    1   \n",
            "13                             A Confederacy of Dunces                    1   \n",
            "444   The Sleepwalkers: How Europe Went to War in 1914                    1   \n",
            "\n",
            "     total_tokens_in_book  total_highlights_in_book  \n",
            "332                 40912                       342  \n",
            "334                 28715                       314  \n",
            "229                 27797                       261  \n",
            "443                 27752                       194  \n",
            "206                 25071                       287  \n",
            "..                    ...                       ...  \n",
            "20                      2                         1  \n",
            "61                      2                         1  \n",
            "96                      1                         1  \n",
            "13                      1                         1  \n",
            "444                     1                         1  \n",
            "\n",
            "[513 rows x 4 columns]\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Output"
      ],
      "metadata": {
        "id": "1fnohzo4rwWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_stats.to_csv('book_stats.csv', index=False)"
      ],
      "metadata": {
        "id": "6yB2imGBrvMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Tokens count and Words count"
      ],
      "metadata": {
        "id": "skG_E8eBt63t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sum of 'total_tokens_in_book' and 'total_words_in_book'\n",
        "total_tokens_sum = book_stats['total_tokens_in_book'].sum()\n",
        "total_words_sum = book_stats['total_words_in_book'].sum()\n",
        "\n",
        "# Format the sums with thousands separators\n",
        "formatted_total_tokens = f\"{total_tokens_sum:,}\"\n",
        "formatted_total_words = f\"{total_words_sum:,}\"\n",
        "\n",
        "# Print the formatted sums\n",
        "print(\"\\n--- Totals for Books ---\")\n",
        "print(f\"Total tokens across all books: {formatted_total_tokens}\")\n",
        "print(f\"Total words across all books (simple count): {formatted_total_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxO3wGz5sRAm",
        "outputId": "e5831b69-dc08-4a56-e862-b0cbdcedcb9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Totals for Books ---\n",
            "Total tokens across all books: 1,984,734\n",
            "Total words across all books (simple count): 1,700,234\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}